## Read Modes in Spark

Read modes tell Spark how to handle these bad records.

**1. FAILFAST**

Spark immediately throws an exception when it finds a malformed record. Strictly validates data.

ðŸŸ¢ Use when data quality is critical. Helps identify issues early in pipelines.

**2. PERMISSIVE**

It is the default reading mode. It allows us to process invalid records without stopping the execution.

It converts the bad/missing values into null

In formats like JSON, it stores unparseable records in a special column like `_corrupt_record`, ensuring the job continues without failure.

ðŸŸ¢ Use when you don't want Spark to crash on minor issues.

**3. DROPMALFORMED**

Silently drops rows that do not match the schema. No error, but data loss can occur.

ðŸŸ¢ Use when only clean data is needed, and you can afford to skip bad records.

### Read Modes Example with Malformed Data

Suppose your schema expects an integer column, but your data file has a string "thirty".

```csv
name,age
Alice,25
Bob,thirty
```

- PERMISSIVE â†’ "thirty" becomes null

- DROPMALFORMED â†’ row with "Bob,thirty" is dropped

- FAILFAST â†’ Spark throws an error and stops

## Write Operations

It is considered an action since it triggers computation and then it saves the result to external disk.

| Mode                                     | Behavior                                                     |
| ---------------------------------------- | ------------------------------------------------------------ |
| `"error"` or `"errorifexists"` (default) | Fails if data exists at path                                 |
| `"append"`                               | Adds new data without touching existing data                 |
| `"overwrite"`                            | Replaces existing data (can also overwrite partitioned data) |
| `"ignore"`                               | Does nothing if data exists (no error, just skips write)     |
